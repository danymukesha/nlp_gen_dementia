{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a58f540-77af-4921-afbb-ac2190469b2d",
   "metadata": {},
   "source": [
    "# NLP-based algorithms for genomic literature in dementia subtypes\n",
    "\n",
    "How can Natural Language Processing (NLP)-driven analysis of genomic literature uncover previously unrecognized genetic associations among different dementia subtypes?\n",
    "\n",
    "Current genomic research on dementia primarily investigates individual diseases in isolation, often neglecting the complex, overlapping genetic architecture shared across various subtypes of dementia. This approach limits the potential to identify broader genetic patterns or interactions that might contribute to the pathophysiology of multiple dementia subtypes. NLP-based methods offer an opportunity to systematically analyze vast bodies of literature and extract subtle, yet significant, relationships between genetic factors and diverse forms of dementia. This topic of research analysis aims to address this gap by utilizing the power of NLP algorithms to mine large-scale genomic data within scientific literature, facilitating the identification of novel connections that have previously gone unnoticed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebbfea-b566-4809-94d2-7ea32e0ae701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "# 1_data_collection\n",
    "import pandas as pd\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 2_text_processing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# 3_analysis_visualization.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# 4_advanced_analysis.py\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c077d6-3560-444b-ac8a-df9bd0e73659",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3f3d6-3d07-4f75-83fd-9614500a4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_articles(disease_terms, max_results=1000):\n",
    "    \"\"\"\n",
    "    Fetch PubMed articles related to dementia subtypes\n",
    "    \"\"\"\n",
    "    Entrez.email = \"danymukesha@gmail.com\" \n",
    "    \n",
    "    articles_data = []\n",
    "    \n",
    "    dementia_subtypes = disease_terms\n",
    "    \n",
    "    for subtype, terms in dementia_subtypes.items():\n",
    "        for term in terms:\n",
    "            handle = Entrez.esearch(db=\"pubmed\", \n",
    "                                  term=f\"({term}) AND (genetic OR genomic OR genome-wide)\",\n",
    "                                  retmax=max_results)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            \n",
    "            id_list = record[\"IdList\"]\n",
    "            print(f\"Found {len(id_list)} articles for {term}\")\n",
    "            \n",
    "            # fetching details in batches\n",
    "            for i in range(0, len(id_list), 100):\n",
    "                batch = id_list[i:i+100]\n",
    "                handle = Entrez.efetch(db=\"pubmed\", id=batch, retmode=\"xml\")\n",
    "                articles = Entrez.read(handle)\n",
    "                handle.close()\n",
    "                \n",
    "                for article in articles['PubmedArticle']:\n",
    "                    try:\n",
    "                        article_data = {\n",
    "                            'pmid': article['MedlineCitation']['PMID'],\n",
    "                            'title': article['MedlineCitation']['Article']['ArticleTitle'],\n",
    "                            'abstract': article['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [''])[0],\n",
    "                            'subtype': subtype,\n",
    "                            'search_term': term,\n",
    "                            'publication_date': article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
    "                        }\n",
    "                        articles_data.append(article_data)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                time.sleep(0.5)  # after some claim fro the NCBI servers, i needed to set this up!\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dementia_subtypes = {\n",
    "        \"alzheimer\": [\"Alzheimer's disease\", \"AD\"],\n",
    "        \"vascular\": [\"vascular dementia\", \"VaD\"],\n",
    "        \"lewy\": [\"Lewy body dementia\", \"LBD\"], \n",
    "        \"frontotemporal\": [\"frontotemporal dementia\", \"FTD\"],\n",
    "        \"mixed\": [\"mixed dementia\"]\n",
    "    }\n",
    "    df = fetch_pubmed_articles(dementia_subtypes)\n",
    "    df.to_csv(\"dementia_genomics_articles.csv\", index=False)\n",
    "    print(f\"Collected {len(df)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ebe83-67f5-46e5-95e0-588284e89838",
   "metadata": {},
   "source": [
    "## 2. Text Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb313ca-f7fc-4be9-b36a-785b4a4c8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class GenomicsTextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.genetic_terms = self._load_genetic_terms()\n",
    "    \n",
    "    def _load_genetic_terms(self):\n",
    "        \"\"\"Custom genetic and genomic terminology\"\"\"\n",
    "        return {\n",
    "            'gene', 'mutation', 'variant', 'snp', 'genome', 'genomic', 'genetic',\n",
    "            'allele', 'chromosome', 'locus', 'polymorphism', 'sequencing', 'gwas',\n",
    "            'apoe', 'app', 'psen1', 'psen2', 'mapt', 'grn', 'c9orf72', 'tarDBP',\n",
    "            'expression', 'transcript', 'protein', 'amyloid', 'tau', 'neurofibrillary',\n",
    "            'plaque', 'beta-amyloid', 'phosphorylation', 'biomarker', 'heritability'\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        # rm special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # rm stopwords and short tokens\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_genetic_features(self, text):\n",
    "        \"\"\"Extract genetic-related features from text\"\"\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        tokens = processed_text.split()\n",
    "        \n",
    "        features = {\n",
    "            'genetic_term_count': sum(1 for token in tokens if token in self.genetic_terms),\n",
    "            'unique_genetic_terms': len(set(token for token in tokens if token in self.genetic_terms)),\n",
    "            'total_terms': len(tokens),\n",
    "            'genetic_density': 0\n",
    "        }\n",
    "        \n",
    "        if features['total_terms'] > 0:\n",
    "            features['genetic_density'] = features['genetic_term_count'] / features['total_terms']\n",
    "        \n",
    "        return features\n",
    "\n",
    "def analyze_articles():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    df = pd.read_csv(\"dementia_genomics_articles.csv\")\n",
    "\n",
    "    processor = GenomicsTextProcessor()\n",
    "    \n",
    "    df['processed_text'] = df['abstract'].apply(processor.preprocess_text)\n",
    "    \n",
    "    genetic_features = df['abstract'].apply(processor.extract_genetic_features)\n",
    "    genetic_df = pd.DataFrame(genetic_features.tolist())\n",
    "    \n",
    "    result_df = pd.concat([df, genetic_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_articles()\n",
    "    results.to_csv(\"processed_dementia_articles.csv\", index=False)\n",
    "    print(\"Text processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e485412-8cf6-4161-8dc5-18bd122336e8",
   "metadata": {},
   "source": [
    "## Analysis and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1674aae-8cb3-479d-b6f0-1baf308e0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis():\n",
    "    \"\"\"Perform comprehensive analysis\"\"\"\n",
    "    df = pd.read_csv(\"processed_dementia_articles.csv\")\n",
    "    \n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "    print(f\"Articles by subtype:\\n{df['subtype'].value_counts()}\")\n",
    "    \n",
    "    # genetic content analysis by dementia subtype\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # plt_1: genetic term density by subtype\n",
    "    plt.subplot(2, 2, 1)\n",
    "    subtype_order = df.groupby('subtype')['genetic_density'].mean().sort_values(ascending=False).index\n",
    "    sns.boxplot(data=df, x='subtype', y='genetic_density', order=subtype_order)\n",
    "    plt.title('Genetic Content Density by Dementia Subtype')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # plt_2: publication trends\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df['year'] = pd.to_datetime(df['publication_date']).dt.year\n",
    "    yearly_counts = df.groupby(['year', 'subtype']).size().unstack(fill_value=0)\n",
    "    yearly_counts.plot(kind='line', ax=plt.gca())\n",
    "    plt.title('Publication Trends by Dementia Subtype')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    \n",
    "    # plt_3: most common genetic terms\n",
    "    plt.subplot(2, 2, 3)\n",
    "    from collections import Counter\n",
    "    all_text = ' '.join(df['processed_text'].dropna())\n",
    "    word_freq = Counter(all_text.split())\n",
    "    genetic_words = {k: v for k, v in word_freq.items() \n",
    "                    if k in ['gene', 'mutation', 'variant', 'genetic', 'genomic', 'apoe']}\n",
    "    \n",
    "    plt.bar(genetic_words.keys(), genetic_words.values())\n",
    "    plt.title('Most Frequent Genetic Terms')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # plt_4: subtype comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    subtype_stats = df.groupby('subtype').agg({\n",
    "        'genetic_density': 'mean',\n",
    "        'genetic_term_count': 'mean',\n",
    "        'unique_genetic_terms': 'mean'\n",
    "    })\n",
    "    subtype_stats.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Genetic Content Metrics by Subtype')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dementia_genomics_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def topic_modeling(df, n_topics=5):\n",
    "    \"\"\"Perform topic modeling on abstracts\"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    texts = df['processed_text'].dropna().tolist()\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    dtm = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # i am trying to apply the LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    # here we show the topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print(\"\\nDiscovered Topics:\")\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_terms = [feature_names[i] for i in topic.argsort()[-10:][::-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_terms)}\")\n",
    "    \n",
    "    return lda, vectorizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = perform_analysis()\n",
    "    \n",
    "    # topic modeling\n",
    "    if len(df) > 50:  # here only if we have enough articles\n",
    "        lda_model, vectorizer = topic_modeling(df)\n",
    "        \n",
    "        texts = df['processed_text'].fillna('')\n",
    "        dtm = vectorizer.transform(texts)\n",
    "        topic_assignments = lda_model.transform(dtm).argmax(axis=1)\n",
    "        df['dominant_topic'] = topic_assignments\n",
    "        \n",
    "        topic_by_subtype = pd.crosstab(df['subtype'], df['dominant_topic'])\n",
    "        print(\"\\nTopic Distribution by Dementia Subtype:\")\n",
    "        print(topic_by_subtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387e609-d0b3-4a4e-9138-812dcf778875",
   "metadata": {},
   "source": [
    "## some more NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e3364-5216-4351-a955-a143a5adb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_subtype_similarities(df):\n",
    "    \"\"\"Analyze genetic similarities between dementia subtypes\"\"\"\n",
    "    \n",
    "    subtype_texts = df.groupby('subtype')['processed_text'].apply(\n",
    "        lambda x: ' '.join(x.dropna())\n",
    "    ).to_dict()\n",
    "    \n",
    "    # calc. TF-IDF vectors for each subtype\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    subtype_vectors = {}\n",
    "    \n",
    "    for subtype, text in subtype_texts.items():\n",
    "        if len(text) > 100:  # i make sure that there is sufficient text\n",
    "            vector = vectorizer.fit_transform([text])\n",
    "            subtype_vectors[subtype] = vector\n",
    "    \n",
    "    # calc similarity matrix\n",
    "    subtypes = list(subtype_vectors.keys())\n",
    "    similarity_matrix = np.zeros((len(subtypes), len(subtypes)))\n",
    "    \n",
    "    for i, subtype1 in enumerate(subtypes):\n",
    "        for j, subtype2 in enumerate(subtypes):\n",
    "            if i <= j:\n",
    "                sim = cosine_similarity(subtype_vectors[subtype1], \n",
    "                                      subtype_vectors[subtype2])[0][0]\n",
    "                similarity_matrix[i][j] = sim\n",
    "                similarity_matrix[j][i] = sim\n",
    "    \n",
    "    # similarity network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i, subtype1 in enumerate(subtypes):\n",
    "        for j, subtype2 in enumerate(subtypes):\n",
    "            if i < j and similarity_matrix[i][j] > 0.1:  # Threshold\n",
    "                G.add_edge(subtype1, subtype2, weight=similarity_matrix[i][j])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='lightblue')\n",
    "    nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']*5 for u,v in G.edges()])\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    \n",
    "    plt.title('Genetic Content Similarity Network between Dementia Subtypes')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('dementia_similarity_network.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return similarity_matrix, subtypes\n",
    "\n",
    "def identify_research_gaps(df):\n",
    "    \"\"\"Identify potential research gaps\"\"\"\n",
    "    \n",
    "    print(\"\\n=== RESEARCH GAP ANALYSIS ===\")\n",
    "    \n",
    "    # 1_ompare genetic focus across subtypes\n",
    "    genetic_focus = df.groupby('subtype')['genetic_density'].mean().sort_values()\n",
    "    print(f\"\\n1. Genetic Focus Ranking (Lowest to Highest):\")\n",
    "    for subtype, density in genetic_focus.items():\n",
    "        print(f\"   {subtype}: {density:.3f}\")\n",
    "    \n",
    "    # 2_dentify underrepresented connections\n",
    "    from itertools import combinations\n",
    "    \n",
    "    print(f\"\\n2. Potential Cross-Subtype Research Opportunities:\")\n",
    "    \n",
    "    # this would be enhanced with more sophisticated NLP\n",
    "    # for now, i only make a kind of conceptual framework\n",
    "    potential_connections = [\n",
    "        \"Shared genetic risk factors between vascular and Alzheimer's dementia\",\n",
    "        \"Overlap in tau pathology genetics across FTD and Alzheimer's\",\n",
    "        \"Inflammatory pathways common to multiple dementia subtypes\"\n",
    "    ]\n",
    "    \n",
    "    for i, connection in enumerate(potential_connections, 1):\n",
    "        print(f\"   {i}. {connection}\")\n",
    "    \n",
    "    return genetic_focus\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"processed_dementia_articles.csv\")\n",
    "    \n",
    "    similarity_matrix, subtypes = analyze_cross_subtype_similarities(df)\n",
    "    research_gaps = identify_research_gaps(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aebffc-a353-424d-bedd-ab11b7bfb013",
   "metadata": {},
   "source": [
    "## Commercial potential and competitive advantages\n",
    "\n",
    "The approach offers several key advantages. First, it is highly cost-effective, as it utilizes freely available genomic and scientific literature, reducing the need for expensive proprietary datasets. It is also comprehensive, capable of analyzing genetic factors across multiple dementia subtypes simultaneously, which enhances its ability to reveal cross-subtype genetic relationships that might otherwise be overlooked. Furthermore, the method is scalable, meaning it can easily be expanded with additional data and more advanced NLP techniques to deepen its insights.\n",
    "\n",
    "This research has broad applications across various sectors. Pharmaceutical companies can use it to identify novel drug targets that may be applicable to multiple dementia types. Research institutions can leverage it to guide future studies and generate hypotheses for further investigation. Biotech startups could also benefit by informing strategies for developing new diagnostic tools.\n",
    "\n",
    "Expected outcomes from this research include a quantitative analysis of genetic research across dementia subtypes, the identification of previously understudied genetic connections, and a clearer visualization of the research landscape. The project will also produce specific, testable hypotheses that can be validated experimentally, further advancing the field of dementia genomics.\n",
    "\n",
    "While promising, the approach has some limitations. It currently relies on abstract-level text, which restricts its depth compared to full-text analysis. Additionally, it is limited by the scope of PubMed’s database and uses basic NLP methods, which could be enhanced with more sophisticated models. In the future, the research can be expanded by incorporating full-text analysis, integrating pre-trained language models like BioBERT and SciBERT, and connecting with genomic data from databases like the GWAS Catalog. These improvements will enable the development of predictive models to better understand the genetic relationships between genes and dementia subtypes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
